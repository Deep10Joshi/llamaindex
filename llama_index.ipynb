{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhEf5_562J2J",
        "outputId": "ab009f59-8805-4681-8439-d1a0b33c0bd5"
      },
      "outputs": [],
      "source": [
        "pip install llama_index llama_index.embeddings.huggingface llama_index.llms.huggingface chromadb llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXqBtJPfJuXk",
        "outputId": "cd8aec35-a3ca-48e5-998f-a92d9be2c372"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from ast import parse\n",
        "from typing import List\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    Settings,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "from llama_index.core.node_parser import SimpleFileNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI, HuggingFaceLLM, TextGenerationInference\n",
        "from llama_index_client import Document\n",
        "from pydantic import BaseModel\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.vector_stores import SimpleVectorStore\n",
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "\n",
        "# import huggingface_hub\n",
        "# huggingface_hub.login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG9u_-xZ1zyk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Offers(BaseModel):\n",
        "    \"\"\"Data model of Offers\"\"\"\n",
        "\n",
        "    offering_id: List[str]\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__ (self):\n",
        "        self.LLM_MODEL = HuggingFaceInferenceAPI(\n",
        "            model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "            # model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "            # model_name=\"01-ai/Yi-1.5-34B-Chat\",\n",
        "            # model_name=\"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "            # model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "            # model_name=\"google/gemma-1.1-2b-it\",\n",
        "            token=\"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\",\n",
        "        )\n",
        "\n",
        "        self.EMBED_MODEL = HuggingFaceEmbedding(\n",
        "            # model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "            # model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "\n",
        "            model_name=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "            # model_name=\"intfloat/multilingual-e5-large\",  #embeddings not generated till now\n",
        "\n",
        "            # model_name=\"nvidia/dragon-multiturn-context-encoder\", # dont use nvidia/dragon-multiturn-context-encoder for query generation, its only for embeddings generation, instead use query-encoder for querrying\n",
        "\n",
        "            # model_name=\"nvidia/dragon-multiturn-query-encoder\",\n",
        "\n",
        "            text_instruction=\"Given are the offers we provide, where each offer is uniqely identified by its offering_id\",\n",
        "            query_instruction=\"Retrieve all the relevent offering_ids from the given query\"\n",
        "        )\n",
        "\n",
        "\n",
        "        self.PERSIST_DIR = \"./vector-indexes/\" + str(self.EMBED_MODEL.model_name)\n",
        "        self.DATA_DIR = \"./\"\n",
        "        self.VECTOR_INDEX = None\n",
        "\n",
        "\n",
        "\n",
        "        self.CHROMA_DB = chromadb.Client()\n",
        "        self.CHROMA_COLLECTION = self.CHROMA_DB.get_or_create_collection(\"Categories\")\n",
        "\n",
        "\n",
        "\n",
        "        Settings.embed_model = self.EMBED_MODEL\n",
        "        Settings.llm = self.LLM_MODEL\n",
        "        # Settings.num_output = 2\n",
        "\n",
        "\n",
        "    def notifyMessage(self, text):\n",
        "        print(f\"{'='*10}{text}{'='*10}\")\n",
        "\n",
        "\n",
        "    def createDocs(self):\n",
        "        # Builds nodes from the documents\n",
        "        self.docs = SimpleDirectoryReader(self.DATA_DIR).load_data(show_progress=True)\n",
        "        parser = SentenceSplitter(paragraph_separator=\"},\\n\")\n",
        "        self.nodes = parser.get_nodes_from_documents(self.docs, show_progress=True)\n",
        "        self.notifyMessage(f\"Parsed {len(self.nodes)} Nodes\")\n",
        "\n",
        "    def saveVectorIndexToDisk(self):\n",
        "        # Saves the index into persist directory\n",
        "        self.VECTOR_INDEX.storage_context.persist(persist_dir=self.PERSIST_DIR)\n",
        "        self.notifyMessage(\"Vector Index Saved\")\n",
        "\n",
        "\n",
        "\n",
        "    def simpleChromaDB(self):\n",
        "        # import chromadb.utils.embedding_functions as embedding_functions\n",
        "        # huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
        "        #     api_key=\"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\",\n",
        "        #     model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "        # )\n",
        "        client = chromadb.PersistentClient()\n",
        "        self.CHROMA_COLLECTION = client.get_or_create_collection(\"Categories\")\n",
        "\n",
        "        file = open(\"./about.txt\", \"r\")\n",
        "        self.nodes = json.load(file)\n",
        "\n",
        "        print(\"file read\")\n",
        "        counter=1\n",
        "\n",
        "        for i in self.nodes[\"data\"]:\n",
        "            counter+=1\n",
        "\n",
        "            curr_metadata = {\"offering_id\": \"\", \"tag\": \"\", \"subtags\": \"\"}\n",
        "            curr_id = i[\"offering_id\"]\n",
        "            curr_metadata[\"offering_id\"] = i[\"offering_id\"]\n",
        "            curr_embeddings = self.EMBED_MODEL.get_text_embedding(str(i))\n",
        "\n",
        "            for tag_i in i[\"tag\"]:\n",
        "                curr_metadata[\"tag\"] = str(tag_i[\"name\"])\n",
        "                for sub_i in tag_i[\"subtags\"]:\n",
        "                    curr_metadata[\"subtags\"] = str(sub_i[\"name\"])\n",
        "\n",
        "            self.CHROMA_COLLECTION.add(\n",
        "                documents=[str(i)],\n",
        "                embeddings=[curr_embeddings],\n",
        "                metadatas=[curr_metadata],\n",
        "                ids=[curr_id]\n",
        "            )\n",
        "\n",
        "            if counter%50 == 0:\n",
        "                print(\"parsed \", counter, \" nodes\")\n",
        "\n",
        "    def testChromaDB(self, userPrompt, top_res=5):\n",
        "        client = chromadb.PersistentClient()\n",
        "        self.CHROMA_COLLECTION = client.get_or_create_collection(\"Categories\")\n",
        "        print(self.CHROMA_COLLECTION.count())\n",
        "\n",
        "        text = self.EMBED_MODEL.get_query_embedding(userPrompt)\n",
        "\n",
        "        ans = self.CHROMA_COLLECTION.query(\n",
        "            query_embeddings=[text],\n",
        "            n_results=top_res,\n",
        "            include=[\"documents\", \"distances\", \"metadatas\"],\n",
        "            where={\"tag\": f\"{userPrompt}\"}\n",
        "        )\n",
        "        print(ans)\n",
        "        for i in ans:\n",
        "            print(i, \"   \", ans[i])\n",
        "\n",
        "\n",
        "# ====================Chroma DB======================\n",
        "    def createIndexFromChromaStorage(self):\n",
        "        self.createDocs()\n",
        "\n",
        "        # Creating chroma vector store\n",
        "        chroma_vector_store = ChromaVectorStore(self.CHROMA_COLLECTION, persist_dir=self.PERSIST_DIR)\n",
        "\n",
        "        # creating a storage context out of the chroma vector store\n",
        "        chroma_storage_context = StorageContext.from_defaults(vector_store=chroma_vector_store)\n",
        "\n",
        "        # Building the index by using the storage context\n",
        "        self.VECTOR_INDEX = VectorStoreIndex(\n",
        "            nodes=self.nodes,\n",
        "            storage_context=chroma_storage_context,\n",
        "            show_progress=True,\n",
        "            embed_model=self.EMBED_MODEL\n",
        "        )\n",
        "\n",
        "        # Saving the index into the disk\n",
        "        self.saveVectorIndexToDisk()\n",
        "\n",
        "    def loadIndexFromChromaStorage(self):\n",
        "        # Creating chroma vector store\n",
        "        chroma_vector_store = ChromaVectorStore(self.CHROMA_COLLECTION, persist_dir=self.PERSIST_DIR)\n",
        "\n",
        "        # Building the index from the loaded vector store\n",
        "        self.VECTOR_INDEX = VectorStoreIndex.from_vector_store(\n",
        "            chroma_vector_store,\n",
        "            embed_model=self.EMBED_MODEL\n",
        "        )\n",
        "        self.notifyMessage(\"Chroma Vector Index Loaded\")\n",
        "\n",
        "\n",
        "# ==================== Default DB ========================\n",
        "    def createIndexFromDefaultStorage(self):\n",
        "        self.createDocs()\n",
        "        self.VECTOR_INDEX = VectorStoreIndex(\n",
        "            nodes=self.nodes,\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        # Saving the index into the disk\n",
        "        self.saveVectorIndexToDisk()\n",
        "\n",
        "\n",
        "    def loadIndexFromDefaultStorage(self):\n",
        "        # creating a storage context from defaults\n",
        "        storage_context = StorageContext.from_defaults(\n",
        "            persist_dir=self.PERSIST_DIR,\n",
        "        )\n",
        "\n",
        "        # Building the index from the storage context\n",
        "        self.VECTOR_INDEX = load_index_from_storage(\n",
        "            storage_context=storage_context\n",
        "        )\n",
        "        self.notifyMessage(\"Vector Index Loaded\")\n",
        "\n",
        "\n",
        "    def generateResponse(self, userPrompt:str):\n",
        "        template = (\n",
        "            \"We have provided context information below. \\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"You are a JSON search engine whose role is to find all the offering id's of offers where the details or the tag matches content of the question being asked\"\n",
        "            \"\\n---------------------\\n\"\n",
        "            \"You should not provide any extra details, only retrieve the offering_id\"\n",
        "            \"\\n---------------------\\n\"\n",
        "            \"The output should be in this format: '[offering id's]' \"\n",
        "            \"\\n---------------------\\n\"\n",
        "            \"Given this information, please answer the question: give various offer details for {query_str}\\n\"\n",
        "        )\n",
        "        qa_template = PromptTemplate(template)\n",
        "        message = qa_template.format_messages(query_str=userPrompt)[0].content\n",
        "        # print(message)\n",
        "\n",
        "        # print(self.VECTOR_INDEX.as_retriever(similarity_top_k=2).retrieve(userPrompt))\n",
        "        self.QUERY_ENGINE = self.VECTOR_INDEX.as_query_engine(\n",
        "            similarity_top_k=5,\n",
        "        )\n",
        "        response = self.QUERY_ENGINE.query(message)\n",
        "        # print(response)\n",
        "        return response\n",
        "\n",
        "    def generateResponseFromClassifier(self, userPrompt:str):\n",
        "        classifier = pipeline(\"zero-shot-classification\",\n",
        "                        model=\"facebook/bart-large-mnli\")\n",
        "        \n",
        "        sequence_to_classify = f\"{userPrompt}\"\n",
        "        candidate_labels = [\n",
        "            'Dining', \n",
        "            'Shopping', \n",
        "            'Travel', \n",
        "            'Electronix Entertainment', \n",
        "            'Beauty and Wellness', \n",
        "            'Gifting', \n",
        "            'Health & Fitness',\n",
        "        ]\n",
        "\n",
        "        ans = classifier(sequence_to_classify, candidate_labels)\n",
        "        print(ans)\n",
        "        return ans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RkDI3jdx5TxG",
        "outputId": "9f9dba3c-d4e8-4be8-a0cc-0275994a90ef"
      },
      "outputs": [],
      "source": [
        "l = Model()\n",
        "# l.createDocs()\n",
        "# l.createIndexFromChromaStorage()\n",
        "# l.simpleChromaDB()\n",
        "l.testChromaDB(\"Travel\", 10)\n",
        "# l.loadIndexFromChromaStorage()\n",
        "\n",
        "\n",
        "\n",
        "# l.createVectorIndex()\n",
        "# l.retrieveVectorIndex()\n",
        "# l.generateResponse(\"something sweet to eat\")\n",
        "# l.customPipeline(\"something sweet to eat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRriYNHRxd8a"
      },
      "outputs": [],
      "source": [
        "# a = l.nodes[0].get_content()\n",
        "# print(a)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
